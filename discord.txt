- Make a server
- Make chat input and sned it to ollama container
- Take output and put it in chat
- Maybe give links in future liek chatgpt for more research

from random import choice, randint
import ollama
# Here I just need to ask the AI to give me a response cuz the infinite loop is abstracted by the discord api and async python
messages=[]

def get_response(user_input: str) -> str:
    # Replace all of this logic with our own
    # lowered: str = user_input.lower()                                                                       # This contains a log of messages from the conversation

    message={"role":"user","content":f"{user_input}"} # THis is the input                # Create a new message
    if message["content"] == "quit": 
        return "Goodbye!"     
                                           # Break the loop if the message is 'quit
    messages.append(message)       # This is the input history    
      
                                                # Append message to the log
    try:                                            
        stream=ollama.chat(model="llama3:8b",messages=messages,stream=True)                 # Call the Ollama server and return a stream
        ai_response=[]                                                                      # This contains AI Response chunks
        for chunk in stream:                                                                # Iterate through the stream
            content=chunk["message"]["content"]                                             # Get the content from the server response
            ai_response.append(content)                                                     # Append the content to the AI Response
            print(content,end='',flush=True)                                                  # Print each chunk in the stream
        # messages.append({"role": "assistant", "content": full_response})
        # return full_response  
    except Exception as e:
        print(f"Error: {e}")
        return "Sorry, there was an error processing your request."